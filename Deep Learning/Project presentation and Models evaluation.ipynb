{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project presentation and Models evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "In this study, we want to classify songs to their respective genre, based on their lyrics with Deep Learning models.\n",
    "In previous research, we have attempted the same task, but with machine Learning algorithms -Naive Bayes, Random Forest, and Ada Boost Classifiers. The highest score that we achieved with Ada Boost (with logistic Regression) was 0.46.\n",
    "\n",
    "We assume that with NN model, we can achieve a higher score. For this purpose, we used different model architectures, model and hyperparameters optimizations.\n",
    "\n",
    "Best LSTM model achieved accuracy on validation dataset of 0.62 and Best CNN model - 0.58."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Music has always played an integral role in human lives ever since the emerging of civilizations and even before - when humans were leaving in tribes. It is a form of art, a way to express one's feelings, a way to convey information, storytelling, a social act of bonding people, an entertainment, a history of humans. Different historical eras were marked by the emerging and favoring of different music genres. Take for an example the Renaissance epoch - the rise of humanism and its impact on the secular music or post-world-war period and the wave of protest music such as rock and roll or blues. All these historical events had a huge impact on music - instrumental and text, creating new music genres.\n",
    "\n",
    "Nowadays there are hundreds of music genres based on the different features of the songs, as most of them are stemming from a few main genres. Moreover, artists continue to explore different music styles throughout their careers, making the borders between them very thin or even making them disappear. Rock band Red Hot Chili Peppers employed rapping on some of their songs. Jimi Hendrix, The Rolling Stones, and Aerosmith were mixing rock with punk.\n",
    "\n",
    "\n",
    "In this study, each song only has one genre label. In reality, assigning just one genre to a song can be more challenging when there is some degree of possible overlap between certain genres.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "**1. Preprosessing**\n",
    "\n",
    "In Prepare and preprocess the data - final draft notebook we:\n",
    "\n",
    "* extracted/downloaded the data, cleaned it and analyzed it\n",
    "* only songs with English lyrics are kept\n",
    "* text was preprocessed with NLP toolkit - we chose lemmatazing of the text\n",
    "* we kept lyrics with sequence lenght between 20 and 800 words\n",
    "* not all genres were equally represented, and in order not to work with a highly imbalanced dataset, we kept only those with enough samples: jazz , rock, metal, and pop. \n",
    "* lyrics were split into training and testing (1000 samples) datasets to be processed with keras.\n",
    "\n",
    "**2. NN models**\n",
    "\n",
    "**2.1 Architecture**\n",
    "\n",
    "* We used Keras vecktorize layer to tokenize the preprocessed text\n",
    "\n",
    "* We used pretrained GLoVe vectors (6 Billion tokens and 100 dimension)  for embedding. \n",
    "\n",
    "\n",
    "* We used Keras vectorize layer to tokenize the preprocessed text\n",
    "\n",
    "* We used pre trained GLoVe vectors (6 Billion tokens and 100 dimensions)  for embedding. \n",
    "\n",
    "* The base model was LSTM model with one layer, then we optimized it by adding an additional layer, dropout, and maxpooling, we experimented with Bi-directional LSTM with 1 layer and with an optimized Bi-directional model with an additional layer. Then we compared the performance of LSTM models with multilayer CNN model.\n",
    "\n",
    "\n",
    "**2.2 Hyperparameters**\n",
    "\n",
    "* batch size - 8, 6\n",
    "* vocabulary size - 5000, 1000\n",
    "* maximum sequence length - 300, 250, 200\n",
    "* Adam learning rate - 1e-2, 1e-3 ( default learning rate), 1e-4, 1e-6\n",
    "\n",
    "\n",
    "**2.3 Metrics** - accuracy score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "kaggle datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best CNN model**\n",
    "\n",
    "**Version 2**\n",
    "* BATCH_SIZE = 16 \n",
    "* MAX_VOCABULARY_SIZE = 10000 \n",
    "* MAX_OUTPUT_LENGTH = 300\n",
    "* EMBEDDING_DIMENSIONS = 100 #loaded GloVe trained  with 100d\n",
    "* LEARNING_RATE = 1e-6\n",
    "\n",
    "Both (with and without droput rate) CNN models were performing steady, model was learning during training - validation and training accuracy were relatively the same, which might mean high bias, but the model was learning and not overfitting, unlike with the others experiments that we had (version 3 and version 4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"version_2_cnn.png\" width=\"500\"/>  \n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"version_2_cnn_drop_out.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best LSTM model**\n",
    "\n",
    "With just 10 epochs we achived the highest validation accuracy of the optimized LSTM 62.4\n",
    "\n",
    "**Version3**\n",
    "\n",
    "* BATCH_SIZE = 8 \n",
    "* MAX_VOCABULARY_SIZE = 5000 \n",
    "* MAX_OUTPUT_LENGTH = 200\n",
    "* EMBEDDING_DIMENSIONS = 100 #loaded GloVe trained  with 100d\n",
    "* LEARNING_RATE = 1e-4 \n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"version_3_lstm_optimized.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"version_3_lstm_bi.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "With NN models we achived higher accuracy score than with machine learning Ada boost Classifier.\n",
    "\n",
    "\n",
    "**Furure work:**\n",
    "\n",
    "* feed to the network not whole sequences of text ( whole song), but sentences instead\n",
    "\n",
    "* try with GloVe 300 dimensions embedding\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
